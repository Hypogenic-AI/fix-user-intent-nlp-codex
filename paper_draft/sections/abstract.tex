\section*{Abstract}
\label{sec:abstract}
We study whether LLM-based query rewriting preserves user intent and whether clarification gating can improve intent fidelity without excessive questioning. We evaluate four policies on a 120-example, context-heavy sample from the \qrecc test split: \norewrite, \direct, \always, and \gated. We score intent preservation using gold-rewrite overlap (BLEU-1, ROUGE-L), semantic similarity (SBERT cosine), and an LLM judge (1--5), and we report clarification rate as a measure of user burden. \direct achieves the highest LLM-judge score (4.725) and strong semantic similarity (0.851). \gated reduces clarification rate to 56.7\% but does not improve intent preservation; its LLM-judge score is slightly worse than \direct (4.567, $p=0.046$). \always clarifies every example and performs worst on intent fidelity metrics. These results suggest that naive ambiguity-based gating can reduce questions without improving intent preservation, and may even introduce small regressions when the model already resolves context correctly. Our evaluation framework clarifies this trade-off and motivates future work on calibrated ambiguity detection and human-in-the-loop clarification.
