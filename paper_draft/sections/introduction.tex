\section{Introduction}
\label{sec:introduction}
Assistant systems rewrite user queries to be helpful, but even small changes can shift intent and erode trust. In conversational settings, the risk is higher because questions depend on context and entities that may be implicit. A system that ``helpfully'' rewrites a query can still be wrong in ways that are hard to detect and frustrating to users.

Intent preservation is therefore a core requirement for query rewriting in search and assistants. At the same time, asking clarifying questions can add friction. The central product tension is clear: avoid wrong rewrites without annoying users with unnecessary clarification.

Prior work on conversational question rewriting focuses on rewriting quality or downstream retrieval and QA performance~\cite{elgohary2019canard,anantha2021qrecc,mo2023convgqr,wu2021conqrr}. Clarification-question research is often studied separately~\cite{kumar2020clarq}. As a result, we lack direct evidence about whether clarification helps intent preservation in rewriting pipelines and how much user burden it introduces.

We examine this gap with a controlled comparison of four policies: \norewrite, \direct, \always, and \gated. Our approach evaluates intent preservation with lexical overlap, semantic similarity, and an LLM-based judge, and it measures clarification rate as a user-burden proxy. \figref{fig:method} summarizes the evaluation workflow.

Quantitatively, \direct achieves the highest LLM-judge score (4.725), while \gated reduces clarifications to 56.7\% but slightly reduces LLM-judge fidelity (4.567, $p=0.046$). \always clarifies every example and performs worst on intent metrics. These results show that naive ambiguity gating reduces questions without improving intent preservation.

In summary, our main contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item We propose an intent-preservation evaluation that combines gold-rewrite overlap, semantic similarity, and LLM judging with a clarification-rate cost.
    \item We conduct a controlled comparison of rewrite and clarification policies on a context-heavy \qrecc sample.
    \item We quantify the clarification-rate vs. intent-fidelity trade-off and show that naive gating can reduce questions while slightly degrading intent fidelity.
\end{itemize}
